{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer\n",
    "\n",
    "> In this notebook, we will go through a basic implementation of a basic Vision Transformer in `PyTorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import einops as ein\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exctracting patches and projecting to embedding space\n",
    "\n",
    "> The first step will be breaking the image into patches and projecting them into the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Breaks an input image into patches and projects them into an embedding space.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 patch_size, # Patch size. As as starting point, we'll be using squared patches.\n",
    "                 d_emb, # Embedding dim.\n",
    "                 in_channels=3, # Image channels.\n",
    "                 ):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size if isinstance(patch_size, tuple) else (patch_size, patch_size)\n",
    "        self.d_emb = d_emb\n",
    "\n",
    "        self.embedding = nn.Linear(torch.multiply(*self.patch_size)*in_channels, d_emb)\n",
    "\n",
    "    def forward(self,\n",
    "                inputs,\n",
    "                ):\n",
    "\n",
    "        ## 1. Break image into patches and flatten them putting together the batch and patch dims\n",
    "        patches = ein.rearrange(inputs, \"b c (h h2) (w w2) -> b (h w) (c h2 w2)\", h2=self.patch_size[0], w2=self.patch_size[1])\n",
    "        n_patches = patches.shape[1]\n",
    "        \n",
    "        ## 1.1. Put together the batch and patch dims\n",
    "        patches = ein.rearrange(patches, \"batch patches pixels -> (batch patches) pixels\")\n",
    "\n",
    "        ## 2. Project them into embedding space\n",
    "        patches_emb = self.embedding(patches)\n",
    "\n",
    "        ## 3. Recover the patch dim\n",
    "        patches_emb = ein.rearrange(patches_emb, \"(batch patches) d_emb -> batch patches d_emb\", patches=n_patches)\n",
    "\n",
    "        return patches_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 588])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_size = (14, 14)\n",
    "batch_size = 4\n",
    "d_emb = 50\n",
    "sample_input = torch.randn(size=(batch_size,3,28,28))\n",
    "patches = ein.rearrange(sample_input, \"b c (h h2) (w w2) -> (b h w) (c h2 w2)\", h2=patch_size[0], w2=patch_size[1])\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 50])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PatchEmbedding(patch_size=patch_size, in_channels=3, d_emb=d_emb)\n",
    "sample_pe = pe(sample_input)\n",
    "sample_pe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Token\n",
    "\n",
    "> To be able to perform classification tasks, the *ViT* requieres a class token [DSExchange Explanation](https://datascience.stackexchange.com/questions/90649/class-token-in-vit-and-bert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassToken(nn.Module):\n",
    "    \"\"\"Prepends a CLASS token to the embedding the space to perform classification with a ViT.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_emb, # Embedding dim.\n",
    "                 ):\n",
    "        super(ClassToken, self).__init__()\n",
    "        self.cls_token = nn.Parameter(torch.zeros(d_emb))\n",
    "    \n",
    "    def forward(self,\n",
    "                inputs, # Embeddings tensor [Batch, N_patch, D_emb] to add a class token.\n",
    "                ):\n",
    "        ## 0. Extract batch_size from input\n",
    "        batch_size, _, _ = inputs.shape\n",
    "\n",
    "        ## 1. Add fake dims to be able to concatenate the class token with the embedding tensor.\n",
    "        cls_token = ein.repeat(self.cls_token, \"d_emb -> batch n_patches d_emb\", batch=batch_size, n_patches=1)\n",
    "\n",
    "        ## 2. Concatenate the class Token at the beggining of the sequence.\n",
    "        return torch.cat((cls_token, inputs), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 50])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = ClassToken(d_emb=d_emb)\n",
    "sample_pec = ct(sample_pe)\n",
    "sample_pec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position Embedding\n",
    "\n",
    "> To give the model a notion of spatial distribution, we will be including position information for each patch using patch encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "    \"\"\"Includes a position encoding into an embedding.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_emb, # Embedding dim.\n",
    "                 ):\n",
    "        super(PositionEncoding, self).__init__()\n",
    "        self.position_encoding = nn.Parameter(torch.zeros(d_emb))\n",
    "\n",
    "    def forward(self,\n",
    "                inputs,# Embeddings tensor [Batch, N_patch + 1, D_emb] to add a class token.\n",
    "                ):\n",
    "                \n",
    "        ## 1. Add the position encoding to the input vector.\n",
    "        return inputs + self.position_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 50])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose = PositionEncoding(d_emb=d_emb)\n",
    "sample_pecpose = pose(sample_pec)\n",
    "assert (sample_pecpose == sample_pec).all() # It's initialized to 0, so input and output must be equal.\n",
    "sample_pecpose.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cuda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da5141a55de43f9a5c077a362efe5e2ae0cb795b0fc8676e62dbd4f64287ec27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
